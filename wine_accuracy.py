# -*- coding: utf-8 -*-
"""wine accuracy

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_Exdh_750ziBh6IdoyYducu8UXlq4qDD
"""

# Hungarian Algorithm Implementation
def hungarian_algorithm(A, n, m):
    INF = sys.maxsize
    # Potential vectors for row and column adjustments
    u = [0] * (n + 1)
    v = [0] * (m + 1)
    # Assignment tracking
    p = [0] * (m + 1)
    way = [0] * (m + 1)

    for i in range(1, n + 1):
        p[0] = i
        j0 = 0
        minv = [INF] * (m + 1)  # Minimum values for potential updates
        used = [False] * (m + 1)  # Tracks visited columns

        while True:
            used[j0] = True
            i0 = p[j0]
            delta = INF
            j1 = -1

            for j in range(1, m + 1):
                if not used[j]:
                    cur = A[i0][j] - u[i0] - v[j]  # Compute reduced cost
                    if cur < minv[j]:
                        minv[j] = cur
                        way[j] = j0
                    if minv[j] < delta:
                        delta = minv[j]
                        j1 = j

            for j in range(m + 1):
                if used[j]:
                    u[p[j]] += delta  # Update potentials for assigned rows
                    v[j] -= delta  # Update potentials for assigned columns
                else:
                    minv[j] -= delta  # Reduce minimum values for next iteration

            j0 = j1  # Move to next column

            if p[j0] == 0:
                break

        while j0:
            j1 = way[j0]
            p[j0] = p[j1]
            j0 = j1

    # Return final assignments
    return p[1:]

import time  # For measuring execution time
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
from scipy.optimize import linear_sum_assignment
import sys

# Start measuring total execution time
start_time = time.time()

# Load the wine dataset
file_path = "wine_dataset.csv"
df = pd.read_csv(file_path)

# Display the first few rows of the dataset for verification
print(df.head())

# Check current columns of the DataFrame
print("Columns in DataFrame before selecting numeric columns:", df.columns.tolist())

# Ensure only numeric columns remain in the DataFrame
df = df.select_dtypes(include=[np.number])  # Keep only numeric columns

# Define the target column (if it exists)
Classes_column = "Wine"  # Column containing the true class labels

# Ensure the 'classes' column exists before dropping
if Classes_column in df.columns:
    numeric_columns = df.drop(columns=[Classes_column]).columns  # Exclude 'classes'
else:
    print(f"Warning: '{Classes_column}' column not found in the DataFrame. Proceeding with all numeric columns.")
    numeric_columns = df.columns

# Standardize the data (excluding 'classes' if it exists)
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df[numeric_columns])

# Print results for debugging
print(f"Preprocessed Data Shape: {df.shape}")
print(f"Numeric Columns: {numeric_columns.tolist()}")

# Measure time for silhouette score calculation
silhouette_start = time.time()

# Use silhouette score to determine the optimal number of clusters
use_silhouette = True
cluster_range = range(3, 6)  # Test clusters between 3 and 5
n_init = 10
max_iter = 300
silhouette_results = []

# Measure time for silhouette score calculation
silhouette_start = time.time()

if use_silhouette:
    for k in cluster_range:
        kmeans = KMeans(n_clusters=k, init="random", n_init=n_init, max_iter=max_iter)
        kmeans.fit(X_scaled)
        score = silhouette_score(X_scaled, kmeans.labels_)
        silhouette_results.append((k, score))
        print(f"Clusters: {k}, Silhouette Score: {score}")

    # Select the best number of clusters
    best_k, best_score = max(silhouette_results, key=lambda x: x[1])
    print(f"Best number of clusters: {best_k}, Silhouette Score: {best_score}")
else:
    best_k = 3

silhouette_end = time.time()
print(f"Silhouette score calculation time: {silhouette_end - silhouette_start:.4f} seconds")

# Measure time for K-means clustering
kmeans_start = time.time()

# Run K-Means with the selected number of clusters (`best_k`)
results = []  # To store results of each run
best_inertia = float('inf')  # To track the best inertia (objective measure)
best_labels = None  # To track the best cluster labels
best_run = None  # To track the run that results in the best model

print("\nRunning K-Means 100 times to find the best solution...")

# Measure time for multiple K-Means runs
kmeans_multi_start = time.time()

for run in range(100):
    print(f"Run {run + 1}:")

    # Run K-Means with random initialization
    kmeans = KMeans(n_clusters=best_k, init="random", n_init=1, max_iter=max_iter, random_state=None)
    kmeans.fit(X_scaled)

    # Store results (e.g., inertia, cluster centers, labels)
    results.append({
        "run": run + 1,
        "inertia": kmeans.inertia_,
        "cluster_centers": kmeans.cluster_centers_,
        "labels": kmeans.labels_,
    })

    # Check if this run is the best (lowest inertia)
    if kmeans.inertia_ < best_inertia:
        best_inertia = kmeans.inertia_
        best_labels = kmeans.labels_
        best_run = run + 1

# After finding the best run, assign the best cluster labels to the DataFrame
df["Cluster"] = best_labels

kmeans_multi_end = time.time()
print(f"Best run: {best_run} with inertia: {best_inertia}")
print(f"Time taken for multiple K-Means runs: {kmeans_multi_end - kmeans_multi_start:.4f} seconds")

kmeans_end = time.time()
print(f"K-means clustering time: {kmeans_end - kmeans_start:.4f} seconds")

# Measure time for Hungarian algorithm
hungarian_start = time.time()

# Check the columns in the DataFrame
print("Available columns in the DataFrame:", df.columns.tolist())

# Update the column name to match the actual column in the DataFrame
Classes_column = "Wine"  # Correct column name with the appropriate case

# Verify if the 'Classes' column exists
if Classes_column not in df.columns:
    raise KeyError(f"'{Classes_column}' column not found in the DataFrame. Please check the dataset.")

# Applying Hungarian Algorithm for Optimal Matching
hungarian_start = time.time()

# Create cost matrix: rows = true classes, columns = predicted clusters
true_classes = df[Classes_column].unique()
clusters = range(best_k)
cost_matrix = np.zeros((len(true_classes), best_k))

for i, cls in enumerate(true_classes):
    for j in clusters:
        # Count how many times each cluster is assigned to the class
        cost_matrix[i, j] = -len(df[(df[Classes_column] == cls) & (df["Cluster"] == j)])

# Solve the assignment problem (Hungarian Algorithm)
from scipy.optimize import linear_sum_assignment
row_ind, col_ind = linear_sum_assignment(cost_matrix)

# Map clusters to classes based on the optimal assignment
cluster_to_class = {cluster: true_classes[row] for row, cluster in zip(row_ind, col_ind)}
df["Assigned_Class"] = df["Cluster"].map(cluster_to_class)

hungarian_end = time.time()
print(f"Hungarian algorithm time: {hungarian_end - hungarian_start:.4f} seconds")

hungarian_end = time.time()
print(f"Hungarian algorithm time: {hungarian_end - hungarian_start:.4f} seconds")

# Measure time for mapping
mapping_start = time.time()

# Step 1: Establish mappings of clusters to classes based on the most frequent assignments
correct_assignment = {}
classes = df[Classes_column].unique()  # Unique class labels

for class_label in classes:  # Loop through each class
    # Filter rows belonging to the current class
    class_points = df[df[Classes_column] == class_label]

    # Count the occurrences of each cluster within this class
    top_cluster = class_points["Cluster"].value_counts().idxmax()

    # Assign the most frequent cluster to this class
    correct_assignment[class_label] = top_cluster

# Step 2: Create a column mapping clusters to the correct assigned classes
# Reverse the mapping: from clusters to classes
cluster_to_class = {v: k for k, v in correct_assignment.items()}
df["Assigned_Class"] = df["Cluster"].map(cluster_to_class)

# Step 3: Create a match column to indicate if the assigned class matches the true class
df["Match"] = df[Classes_column] == df["Assigned_Class"]

mapping_end = time.time()
print(f"Cluster-to-species mapping and match column creation time: {mapping_end - mapping_start:.4f} seconds")

# Measure time for WCSS computation
wcss_start = time.time()

# Compute WCSS for different values of k
wcss = []
k_values = range(1, 11)  # Testing k from 1 to 10

for k in k_values:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X_scaled)  # Use standardized data
    wcss.append(kmeans.inertia_)  # WCSS (sum of squared distances)

# Display WCSS values
print("\nWCSS values for different k:")
for k, inertia in zip(k_values, wcss):
    print(f"k = {k}, WCSS = {inertia:.4f}")

wcss_end = time.time()
print(f"WCSS computation time: {wcss_end - wcss_start:.4f} seconds")

# Measure time for accuracy calculation
accuracy_start = time.time()

# Check accuracy
df["Match"] = df[Classes_column] == df["Assigned_Class"]
accuracy = df["Match"].mean()
print(f"\nClustering Accuracy (after Hungarian Algorithm): {accuracy:.4f}")

# Display results
print("\nCluster-to-Class Mapping:")
print(cluster_to_class)
print("\nDataFrame Results:")
print(df[[Classes_column, "Cluster", "Assigned_Class", "Match"]].head())

accuracy_end = time.time()
print(f"Accuracy calculation time: {accuracy_end - accuracy_start:.4f} seconds")

# End measuring total execution time
end_time = time.time()
print(f"\nTotal execution time: {end_time - start_time:.4f} seconds")